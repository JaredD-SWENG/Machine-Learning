{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UQOWUSIH9aL",
        "outputId": "04c6af52-e9a1-4140-f27e-d31ab66fc186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema:\n",
            "\n",
            " id                            int64\n",
            "title                        object\n",
            "body                         object\n",
            "answer_count                  int64\n",
            "comment_count                 int64\n",
            "creation_date                object\n",
            "last_activity_date           object\n",
            "last_editor_display_name     object\n",
            "owner_display_name           object\n",
            "owner_user_id               float64\n",
            "post_type_id                  int64\n",
            "score                         int64\n",
            "tags                         object\n",
            "view_count                    int64\n",
            "accepted_answer_id          float64\n",
            "favorite_count              float64\n",
            "last_edit_date               object\n",
            "last_editor_user_id         float64\n",
            "community_owned_date         object\n",
            "dtype: object\n",
            "Number of questions,columns= (20000, 19)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#read json into a dataframe\n",
        "df_idf=pd.read_json(\"/content/stackoverflow-data-idf.json\",lines=True,encoding='utf-8')\n",
        "\n",
        "# print schema\n",
        "print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
        "print(\"Number of questions,columns=\",df_idf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def pre_process(text):\n",
        "\n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "\n",
        "    #remove tags\n",
        "    text=re.sub(\"<!--?.*?-->\",\"\",text)\n",
        "\n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "\n",
        "    return text\n",
        "\n",
        "df_idf['text'] = df_idf['title'] + df_idf['body']\n",
        "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))\n",
        "\n",
        "#show the second 'text' just for fun\n",
        "df_idf['text'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "EplzYQsPJTys",
        "outputId": "42e9be1d-d78b-4262-c0e9-993c9895fd5b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gradle command line p i m trying to run a shell script with gradle i currently have something like this p pre code def test project tasks create test exec commandline bash c bash c my file dir script sh code pre p the problem is that i cannot run this script because i have spaces in my dir name i have tried everything e g p pre code commandline bash c bash c my file dir script sh tokenize commandline bash c bash c my file dir script sh commandline bash c new stringbuilder append bash append c my file dir script sh commandline bash c bash c my file dir script sh file dir file c my file dir script sh commandline bash c bash dir getabsolutepath code pre p im using windows bit and if i use a path without spaces the script runs perfectly therefore the only issue as i can see is how gradle handles spaces p '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_stop_words(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "\n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return list(frozenset(stop_set))\n",
        "\n",
        "#load a set of stop words\n",
        "stopwords=get_stop_words(\"stopwords.txt\")\n",
        "\n",
        "#get the text column\n",
        "docs=df_idf['text'].tolist()\n",
        "\n",
        "#create a vocabulary of words,\n",
        "#ignore words that appear in 85% of documents,\n",
        "#eliminate stop words\n",
        "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\n",
        "word_count_vector=cv.fit_transform(docs)\n",
        "\n",
        "list(cv.vocabulary_.keys())[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkSPFGl7JzyW",
        "outputId": "f8036429-5ccd-4517-c4ba-49d584a935b2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['serializing',\n",
              " 'private',\n",
              " 'struct',\n",
              " 'public',\n",
              " 'class',\n",
              " 'contains',\n",
              " 'properties',\n",
              " 'string',\n",
              " 'serialize',\n",
              " 'attempt']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "3r7v7NM5M4Q1",
        "outputId": "0e776a2c-ff53-413f-c143-18f7993cc6f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read test docs into a dataframe and concatenate title and body\n",
        "df_test=pd.read_json(\"stackoverflow-test.json\",lines=True)\n",
        "print(\"Number of questions,columns=\",df_test.shape)\n",
        "df_test['text'] = df_test['title'] + df_test['body']\n",
        "df_test['text'] =df_test['text'].apply(lambda x:pre_process(x))\n",
        "\n",
        "# get test docs into a list\n",
        "docs_test=df_test['text'].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCtXeIwgNMA9",
        "outputId": "d79f021c-e07f-4224-c7fb-fbfd9954eb4f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of questions,columns= (500, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "\n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    # word index and corresponding tf-idf score\n",
        "    for idx, score in sorted_items:\n",
        "\n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "_g7jm2CLNxoc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you only needs to do this once, this is a mapping of index to\n",
        "feature_names=cv.get_feature_names_out()\n",
        "\n",
        "# get the document that we want to extract keywords from\n",
        "doc=docs_test[0]\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "#extract only the top n; n here is 10\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "# now print the results\n",
        "print(\"\\n=====Doc=====\")\n",
        "print(doc)\n",
        "print(\"\\n===Keywords===\")\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMdJgc_jNPss",
        "outputId": "377798fd-ebd6-4e79-adb8-0561d368e8f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====Doc=====\n",
            "integrate war plugin for m eclipse into eclipse project p i set up a small web project with jsf and maven now i want to deploy on a tomcat server is there a possibility to automate that like a button in eclipse that automatically deploys the project to tomcat p p i read about a the a href http maven apache org plugins maven war plugin rel nofollow noreferrer maven war plugin a but i couldn t find a tutorial how to integrate that into my process eclipse m eclipse p p can you link me to help or try to explain it thanks p \n",
            "\n",
            "===Keywords===\n",
            "eclipse 0.49\n",
            "maven 0.451\n",
            "war 0.393\n",
            "plugin 0.265\n",
            "integrate 0.233\n",
            "tomcat 0.223\n",
            "project 0.197\n",
            "automate 0.13\n",
            "jsf 0.125\n",
            "possibility 0.121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the document that we want to extract keywords from\n",
        "doc=docs_test[1]\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "#extract only the top n; n here is 10\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "# now print the results\n",
        "print(\"\\n=====Doc=====\")\n",
        "print(doc)\n",
        "print(\"\\n===Keywords===\")\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZCQZjvuSkjj",
        "outputId": "bc5e7c8b-6f78-4f80-883e-53bc7f74714e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====Doc=====\n",
            "phantomjs node page evaulate seems to hang p i have an implementation of waitfor with phantomjs node and it seems that the code sitepage evaluate code has a big lag compared to when it should evaluate true you ll see below that i m logging out the content value and the content logs with what should evaluate as true but this doesn t seem to occur for a good seconds or so after the fact p p any idea what would cause this delay or if there s a better way to evaluate p pre code let promise require bluebird let phantom require phantom let sitepage let phinstance phantom create then instance gt phinstance instance return instance createpage then page gt sitepage page return page open https thepiratebay org search game then status gt return waituntil function this returns the correct content after a short period while the evaluate ends up taking maybe s longer after this content should evaluate true sitepage property content then content gt console log content return sitepage evaluate function return document getelementbyid searchresult then function return sitepage property content catch promise timeouterror function e sitepage close phinstance exit then content gt console log content console log content sitepage close phinstance exit catch error gt console log error phinstance exit var waituntil asynctest gt return new promise function resolve reject function wait console log waiting asynctest then function value if value console log resolve resolve else settimeout wait catch function e console log error found rejecting e reject wait code pre \n",
            "\n",
            "===Keywords===\n",
            "evaluate 0.474\n",
            "content 0.403\n",
            "console 0.281\n",
            "log 0.265\n",
            "function 0.215\n",
            "promise 0.2\n",
            "return 0.195\n",
            "wait 0.169\n",
            "let 0.163\n",
            "resolve 0.156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the document that we want to extract keywords from\n",
        "doc=docs_test[2]\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "#extract only the top n; n here is 10\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "# now print the results\n",
        "print(\"\\n=====Doc=====\")\n",
        "print(doc)\n",
        "print(\"\\n===Keywords===\")\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LPS3rdCThNq",
        "outputId": "54286797-2e28-4e25-858b-246ba3109434"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====Doc=====\n",
            "dynamic operations can only be performed in homogenous appdomain p i m working with an api that requires p pre code lt trust level full legacycasmodel true gt code pre p to be set in my web config file in order to work this works without a problem in vs but when i use vs i get an error stating that dynamic operations can only be performed in homogeneous appdomain the project is targeting net framework and is an asp net mvc project p p i tried changing legacycasmodel to false but then i can t access the object i need not sure how to resolve this issue how can i use legacycasmodel true in vs with dynamic expressions p \n",
            "\n",
            "===Keywords===\n",
            "appdomain 0.41\n",
            "dynamic 0.384\n",
            "performed 0.332\n",
            "operations 0.297\n",
            "targeting 0.199\n",
            "trust 0.182\n",
            "net 0.182\n",
            "project 0.179\n",
            "stating 0.178\n",
            "expressions 0.167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   How many samples are in the training set?<br>\n",
        "There are 20,000 samples in the traning set.\n",
        "2.   How many samples are in the test set?<br>\n",
        "There are 500 samples in the test set.\n",
        "3.   What is this data set?<br>\n",
        "The data set is about StackOverflow posts, with 19 specific fields such as answer count, body, comment count, title, creation date, tags, score, viewc count, etc. We were interested in the title and body.\n",
        "4.   Use the code from the tutorial to obtain these answers:  What are the top 10 keywords and their TF-IDF scores from the test set sample 1 (the document about eclipse project)?  How about sample 2 (the one about phantomjs)?  How about sample 3 (the one about dynamic operations)?<br>\n",
        "     1.  eclipse (0.49), maven (0.451), war (0.393), plugin (0.265), integrate (0.233), tomcat (0.223), project (0.197), automate (0.13), jsf (0.125), possibility (0.121)\n",
        "     2. evaluate (0.474), content (0.403), console (0.281), log (0.265), function (0.215), promise (0.2), return (0.195), wait (0.169), let (0.163), resolve (0.156)\n",
        "     3. appdomain (0.41), dynamic (0.384), performed (0.332), operations (0.297), targeting (0.199), trust (0.182), net (0.182), project (0.179), stating (0.178), expressions (0.167)"
      ],
      "metadata": {
        "id": "-Q5AVYUFOmBp"
      }
    }
  ]
}